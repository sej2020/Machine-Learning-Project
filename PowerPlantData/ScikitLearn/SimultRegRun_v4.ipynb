{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First 20 Models Simultaneous Run**\n",
    "\n",
    "General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import all_estimators\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Importing All Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = all_estimators(type_filter='regressor')\n",
    "\n",
    "all_regs = []\n",
    "all_reg_names = []\n",
    "for name, RegressorClass in estimators:\n",
    "    try:\n",
    "        #these are regressions warping the figures with bad performance. can remove this line if i have time\n",
    "        if name != 'DummyRegressor' and name != 'GaussianProcessRegressor' and name != 'QuantileRegressor' and name != 'SGDRegressor':\n",
    "            print('Appending', name)\n",
    "            reg = RegressorClass()\n",
    "            all_regs.append(reg)\n",
    "            all_reg_names.append(name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(all_regs)\n",
    "print(all_reg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pp_data():\n",
    "    csv_path = os.path.abspath(\"Folds5x2_pp.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "pp = load_pp_data()\n",
    "print(pp.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test Split and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp[\"AT_cat\"] = pd.cut(pp[\"AT\"],bins=[0.,10.,20.,30.,np.inf],labels=[1,2,3,4])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split.split(pp,pp[\"AT_cat\"]):\n",
    "    train_set = pp.loc[train_index]\n",
    "    test_set = pp.loc[test_index]\n",
    "\n",
    "for set_ in(train_set,test_set):\n",
    "    set_.drop(\"AT_cat\",axis=1,inplace=True)\n",
    "\n",
    "pptrain = train_set.copy()\n",
    "pptest = test_set.copy()\n",
    "\n",
    "pptrain_attrib = pptrain.drop(\"PE\",axis=1)\n",
    "pptrain_labels = pptrain[\"PE\"].copy()\n",
    "pptest_attrib = pptest.drop(\"PE\",axis=1)\n",
    "pptest_labels = pptest[\"PE\"].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(pptrain_attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneous Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mother function that runs the models and returns several figures showing comparison of the models\n",
    "def comparison(models,model_names):\n",
    "    cv_data = []\n",
    "    errors = []\n",
    "    passed_models = []\n",
    "    for i in range(len(models)):\n",
    "        x = run(models[i])\n",
    "        if type(x) == dict:\n",
    "            cv_data += [x]\n",
    "        else:\n",
    "            errors += [models[i]]\n",
    "    for j in range(len(models)):\n",
    "        if models[j] not in errors:\n",
    "            passed_models += [model_names[j]]\n",
    "    figs = [test_best(cv_data, passed_models), box_rmse(cv_data, passed_models), box_r2(cv_data, passed_models), box_mae(cv_data, passed_models), runtime(cv_data, passed_models)]\n",
    "    for k in range(len(figs)):\n",
    "        figs[k].savefig(f'fig_{k}.png',bbox_inches='tight')\n",
    "    return test_best(cv_data, passed_models)\n",
    "\n",
    "#the function that performs cross-validation\n",
    "def run(model):\n",
    "    print(f\"checking {model}\")\n",
    "    try:\n",
    "        cv_outer = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "        cv_output_dict = cross_validate(model, pptrain_attrib, pptrain_labels, scoring=[\"neg_mean_squared_error\",\"neg_mean_absolute_error\",\"r2\"], cv=cv_outer, return_estimator=True)\n",
    "        return cv_output_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#vizualizes runtime\n",
    "def runtime(cv_data, passed_models):\n",
    "    timefig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in zip(cv_data,passed_models):\n",
    "        df[j] = list(i[('fit_time')])\n",
    "    sorted_index = df.median().sort_values().index\n",
    "    df_sorted=df[sorted_index]\n",
    "    top20 = df_sorted.drop(columns=df_sorted.columns[20:])\n",
    "    top20_sorted_index = top20.median().sort_values(ascending=False).index\n",
    "    top20_sorted=top20[top20_sorted_index]\n",
    "    top20_sorted.boxplot(vert=False,grid=False)\n",
    "    plt.xlabel('Run Time')\n",
    "    plt.ylabel('Models')\n",
    "    return timefig\n",
    "\n",
    "#vizualizes rmse\n",
    "def box_rmse(cv_data, passed_models):\n",
    "    RMSEfig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in zip(cv_data,passed_models):\n",
    "        df[j] = list(np.sqrt(i['test_neg_mean_squared_error']*-1))\n",
    "    sorted_index = df.median().sort_values().index\n",
    "    df_sorted=df[sorted_index]\n",
    "    top20 = df_sorted.drop(columns=df_sorted.columns[20:])\n",
    "    top20_sorted_index = top20.median().sort_values(ascending=False).index\n",
    "    top20_sorted=top20[top20_sorted_index]\n",
    "    top20_sorted.boxplot(vert=False,grid=False)\n",
    "    plt.xlabel(f'CV Root Mean Squared Error (Lower is better)')\n",
    "    plt.ylabel('Models')\n",
    "    return RMSEfig\n",
    "\n",
    "#vizualizes r-squared\n",
    "def box_r2(cv_data, passed_models):\n",
    "    R2fig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in zip(cv_data,passed_models):\n",
    "        df[j] = list(i['test_r2'])\n",
    "    sorted_index = df.median().sort_values(ascending=False).index\n",
    "    df_sorted=df[sorted_index]\n",
    "    top20 = df_sorted.drop(columns=df_sorted.columns[20:])\n",
    "    top20_sorted_index = top20.median().sort_values().index\n",
    "    top20_sorted=top20[top20_sorted_index]\n",
    "    top20_sorted.boxplot(vert=False,grid=False)\n",
    "    plt.xlabel(f'CV R-Squared Score (Higher is better)')\n",
    "    plt.ylabel('Models')\n",
    "    return R2fig\n",
    "\n",
    "#visualizes mae\n",
    "def box_mae(cv_data, passed_models):\n",
    "    MAEfig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in zip(cv_data,passed_models):\n",
    "        df[j] = list(i['test_neg_mean_absolute_error']*-1)\n",
    "    sorted_index = df.median().sort_values().index\n",
    "    df_sorted=df[sorted_index]\n",
    "    top20 = df_sorted.drop(columns=df_sorted.columns[20:])\n",
    "    top20_sorted_index = top20.median().sort_values(ascending=False).index\n",
    "    top20_sorted=top20[top20_sorted_index]\n",
    "    top20_sorted.boxplot(vert=False,grid=False)\n",
    "    plt.xlabel(f'CV Mean Absolute Error (Lower is better)')\n",
    "    plt.ylabel('Models')\n",
    "    return MAEfig\n",
    "\n",
    "#takes the model produced by the best cv run and runs it over the test data. returns table comparing model performance on test data\n",
    "def test_best(cv_data, passed_models):\n",
    "    rmse = []\n",
    "    r2 = []\n",
    "    mae = []\n",
    "    for i in cv_data:\n",
    "        x = list((np.sqrt(i['test_neg_mean_squared_error']*-1)))\n",
    "        y = list(i['estimator'])\n",
    "        for j in range(len(x)):\n",
    "            if x[j] == min(x):\n",
    "                best = y[j]\n",
    "        predictions = best.predict(pptest_attrib)\n",
    "        rmse += [round(np.sqrt(mean_squared_error(pptest_labels,predictions)),4)]\n",
    "        r2 += [round(r2_score(pptest_labels,predictions),4)]\n",
    "        mae += [round(mean_absolute_error(pptest_labels,predictions),4)]\n",
    "    columnnames = ['rmse','r2','mae']\n",
    "    df = pd.DataFrame(np.array([rmse,r2,mae]).T,index=passed_models,columns=columnnames)\n",
    "    sorted_df = df.sort_values(by=\"rmse\",ascending=True)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    ax.table(cellText=sorted_df.values, rowLabels=sorted_df.index, colLabels=sorted_df.columns, loc='center')\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "y = all_regs\n",
    "y_names = all_reg_names\n",
    "x = all_regs[0:5]\n",
    "x_names = all_reg_names[0:5]\n",
    "comparison(y,y_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e13265c55b2786eb2a2fa7b7b4618c00aafbb52a034e4f1759946ea06f2e244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
