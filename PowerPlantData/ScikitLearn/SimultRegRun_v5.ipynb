{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First 20 Models Simultaneous Run**\n",
    "\n",
    "General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import all_estimators\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Importing All Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = all_estimators(type_filter='regressor')\n",
    "\n",
    "all_regs = []\n",
    "all_reg_names = []\n",
    "for name, RegressorClass in estimators:\n",
    "    try:\n",
    "        if name != 'DummyRegressor' and name != 'GaussianProcessRegressor' and name != 'QuantileRegressor' and name != 'SGDRegressor':\n",
    "            print('Appending', name)\n",
    "            reg = RegressorClass()\n",
    "            all_regs.append(reg)\n",
    "            all_reg_names.append(name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(all_regs)\n",
    "print(all_reg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pp_data():\n",
    "    csv_path = os.path.abspath(\"Folds5x2_pp.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "pp = load_pp_data()\n",
    "print(pp.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test Split and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp[\"AT_cat\"] = pd.cut(pp[\"AT\"],bins=[0.,10.,20.,30.,np.inf],labels=[1,2,3,4])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split.split(pp,pp[\"AT_cat\"]):\n",
    "    train_set = pp.loc[train_index]\n",
    "    test_set = pp.loc[test_index]\n",
    "\n",
    "for set_ in(train_set,test_set):\n",
    "    set_.drop(\"AT_cat\",axis=1,inplace=True)\n",
    "\n",
    "pptrain = train_set.copy()\n",
    "pptest = test_set.copy()\n",
    "\n",
    "pptrain_attrib = pptrain.drop(\"PE\",axis=1)\n",
    "pptrain_labels = pptrain[\"PE\"].copy()\n",
    "pptest_attrib = pptest.drop(\"PE\",axis=1)\n",
    "pptest_labels = pptest[\"PE\"].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(pptrain_attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneous Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mother function that runs the models and returns several figures showing comparison of the models\n",
    "def comparison(models,model_names,metric_list,show):\n",
    "    cv_data = []\n",
    "    errors = []\n",
    "    passed_models = []\n",
    "    if 'neg_mean_squared_error' not in metric_list:\n",
    "        metric_list = metric_list+['neg_mean_squared_error']\n",
    "    for i in range(len(models)):\n",
    "        x = run(models[i],metric_list)\n",
    "        if type(x) == dict:\n",
    "            cv_data += [x]\n",
    "        else:\n",
    "            errors += [models[i]]\n",
    "    for j in range(len(models)):\n",
    "        if models[j] not in errors:\n",
    "            passed_models += [model_names[j]]\n",
    "    figs = [test_best(cv_data, passed_models, metric_list)]\n",
    "    for metric in metric_list:\n",
    "        figs += [boxplot(cv_data, passed_models,metric,show)]\n",
    "    for k in range(len(figs)):\n",
    "        figs[k].savefig(f'fig_{k}.png',bbox_inches='tight')\n",
    "    return test_best(cv_data, passed_models,metric_list)\n",
    "\n",
    "#the function that performs cross-validation\n",
    "def run(model,metric_list):\n",
    "    print(f\"checking {model}\")\n",
    "    try:\n",
    "        cv_outer = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "        cv_output_dict = cross_validate(model, pptrain_attrib, pptrain_labels, scoring=metric_list, cv=cv_outer, return_estimator=True)\n",
    "        return cv_output_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#metric must be a string\n",
    "#show is how many of top models are vizualized\n",
    "def boxplot(cv_data, passed_models, metric, show):\n",
    "    boxfig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    for i,j in zip(cv_data,passed_models):\n",
    "        if metric[:3] == 'neg':\n",
    "            df[j] = list(i['test_'+metric]*-1)\n",
    "        else:\n",
    "            df[j] = list(i['test_'+metric])\n",
    "    if metric == 'r2':\n",
    "        sorted_index = df.median().sort_values(ascending=False).index\n",
    "    else:\n",
    "        sorted_index = df.median().sort_values().index\n",
    "    df_sorted=df[sorted_index]\n",
    "    top20 = df_sorted.drop(columns=df_sorted.columns[show:])\n",
    "    if metric == 'r2':\n",
    "        top20_sorted_index = top20.median().sort_values().index\n",
    "    else:\n",
    "        top20_sorted_index = top20.median().sort_values(ascending=False).index\n",
    "    top20_sorted=top20[top20_sorted_index]\n",
    "    top20_sorted.boxplot(vert=False,grid=False)\n",
    "    plt.xlabel(f'CV {metric}')\n",
    "    plt.ylabel('Models')\n",
    "    return boxfig\n",
    "\n",
    "\n",
    "#takes the model produced by the best cv run and runs it over the test data. returns table comparing model performance on test data\n",
    "def test_best(cv_data, passed_models, metric_list):\n",
    "    metric_columns = []\n",
    "    for metric in metric_list:\n",
    "        metric_columns += [[metric,[]]]\n",
    "    for i in cv_data:\n",
    "        x = list((np.sqrt(i['test_neg_mean_squared_error']*-1)))\n",
    "        y = list(i['estimator'])\n",
    "        for j in range(len(x)):\n",
    "            if x[j] == min(x):\n",
    "                best = y[j]\n",
    "        predictions = best.predict(pptest_attrib)\n",
    "        for k in metric_columns:\n",
    "            #next line won't work. need to figure out how to use statistics module to calculate metrics on test predictions\n",
    "            k[1] += [round(r2_score(pptest_labels,predictions),4)]\n",
    "    columnnames = metric_list\n",
    "    final_columns = []\n",
    "    for m in metric_columns:\n",
    "        final_columns += [m[1]]\n",
    "    df = pd.DataFrame(np.array(final_columns).T,index=passed_models,columns=columnnames)\n",
    "    if metric_list[0] != 'r2':\n",
    "        notr2 = True\n",
    "    else:\n",
    "        notr2 = False\n",
    "    sorted_df = df.sort_values(by=metric_list[0],ascending=notr2)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    ax.table(cellText=sorted_df.values, rowLabels=sorted_df.index, colLabels=sorted_df.columns, loc='center')\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "y = all_regs\n",
    "y_names = all_reg_names\n",
    "x = all_regs[0:5]\n",
    "x_names = all_reg_names[0:5]\n",
    "metric_list = [\"neg_mean_squared_error\",\"neg_mean_absolute_error\",\"r2\"]\n",
    "comparison(x,x_names,metric_list,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e13265c55b2786eb2a2fa7b7b4618c00aafbb52a034e4f1759946ea06f2e244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
