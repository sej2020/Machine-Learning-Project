import pandas as pd
import os
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform as sp_randFloat
from scipy.stats import randint as sp_randInt



def load_pp_data():
    csv_path = r"C:\Users\18123\OneDrive\Documents\IU Bloomington\Machine-Learning-Project\PowerPlantData\CCPP\Folds5x2_pp.csv"
    return pd.read_csv(csv_path)

pp = load_pp_data()
print(pp.describe())

# pp.hist(bins=50, figsize=(15,11))
# plt.show()

pp["AT_cat"] = pd.cut(pp["AT"],bins=[0.,10.,20.,30.,np.inf],labels=[1,2,3,4])
# pp["AT_cat"].hist()
# plt.show()

split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)
for train_index, test_index in split.split(pp,pp["AT_cat"]):
    train_set = pp.loc[train_index]
    test_set = pp.loc[test_index]

for set_ in(train_set,test_set):
    set_.drop("AT_cat",axis=1,inplace=True)

pptrain = train_set.copy()
pptest = test_set.copy()

# corr_matrix = pptrain.corr()
# print(corr_matrix["PE"].sort_values(ascending=False))

# pptrain.plot(kind="scatter",x="AT",y="PE",alpha=0.1)
# pptrain.plot(kind="scatter",x="V",y="PE",alpha=0.1)
# pptrain.plot(kind="scatter",x="AP",y="PE",alpha=0.1)
# pptrain.plot(kind="scatter",x="RH",y="PE",alpha=0.1)
# plt.show()

pptrain_attrib = pptrain.drop("PE",axis=1)
pptrain_labels = pptrain["PE"].copy()

pptest_attrib = pptest.drop("PE",axis=1)
pptest_labels = pptest["PE"].copy()

scaler = StandardScaler()
scaler.fit_transform(pptrain_attrib)
scaler.transform(pptest_attrib)

lin_reg = LinearRegression()
lin_reg.fit(pptrain_attrib,pptrain_labels)

lin_scores = cross_val_score(lin_reg, pptrain_attrib, pptrain_labels, scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)

def display_scores(scores):
    print(f"Mean: {scores.mean()} +- {scores.std()}")
    print("Scores:", scores)

display_scores(lin_rmse_scores)

# print(lin_reg.get_params())

lin_param_grid = [{'copy_X': [True], 'fit_intercept': [True, False], 'n_jobs': [None], 'normalize': ['deprecated'], 'positive': [True,False]}]
lin_grid_search = GridSearchCV(lin_reg, lin_param_grid, cv=10, scoring='neg_mean_squared_error',return_train_score=True)
lin_grid_search.fit(pptrain_attrib,pptrain_labels)
print(lin_grid_search.best_params_)
lin_rmse = lin_grid_search.cv_results_['mean_test_score'][lin_grid_search.best_index_]
print(lin_rmse)
print(np.sqrt(-lin_rmse))

#*****************************************************************************************************************
#*****************************************************************************************************************
print('*'*200)

tree_reg = DecisionTreeRegressor(random_state=10)
tree_reg.fit(pptrain_attrib,pptrain_labels)

tree_scores = cross_val_score(tree_reg, pptrain_attrib, pptrain_labels, scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-tree_scores)

display_scores(tree_rmse_scores)

# print(tree_reg.get_params())

tree_param_grid = [{'ccp_alpha': [0.0,0.00001], 'criterion': ['squared_error','friedman_mse','absolute_error','poisson'], 'max_depth': [None], 'max_features': ['log2'], 'max_leaf_nodes': [9,10,11], 'min_impurity_decrease': [0.0], 'min_samples_leaf': [1], 'min_samples_split': [4], 'min_weight_fraction_leaf': [0.0], 'random_state': [10], 'splitter': ['best']}]
# rand_tree_param_grid = [{'ccp_alpha': sp_randFloat(0,1), 'criterion': ['squared_error','friedman_mse','absolute_error','poisson'], 'max_depth': sp_randInt(2,100), 'max_features': [None,'sqrt','log2'], 'max_leaf_nodes': sp_randInt(2,100), 'min_impurity_decrease': sp_randFloat(0,1), 'min_samples_leaf': sp_randInt(1,100), 'min_samples_split': sp_randInt(2,100), 'min_weight_fraction_leaf': sp_randFloat(0,0.5), 'random_state': [None], 'splitter': ['best','random']}]
tree_grid_search = HalvingGridSearchCV(tree_reg, tree_param_grid, cv=10, factor=3, max_resources=30, scoring='neg_mean_squared_error',return_train_score=True, random_state=15)
# tree_grid_search = RandomizedSearchCV(tree_reg,rand_tree_param_grid,cv=10,n_iter=1)
tree_grid_search.fit(pptrain_attrib,pptrain_labels)
print(tree_grid_search.best_params_)
tree_rmse = tree_grid_search.cv_results_['mean_test_score'][tree_grid_search.best_index_]
print(tree_rmse)
print(np.sqrt(-tree_rmse))

#*******************************************************************************************************************
#*******************************************************************************************************************
print('*'*200)

final_lin = lin_grid_search.best_estimator_
final_tree = tree_grid_search.best_estimator_

final_lin_predictions = final_lin.predict(pptest_attrib)
final_tree_predictions = final_tree.predict(pptest_attrib)

final_lin_mse = mean_squared_error(pptest_labels,final_lin_predictions)
final_lin_rmse = np.sqrt(final_lin_mse)
final_lin_r2 = r2_score(pptest_labels,final_lin_predictions)
final_lin_mae = mean_absolute_error(pptest_labels,final_lin_predictions)

final_tree_mse = mean_squared_error(pptest_labels,final_tree_predictions)
final_tree_rmse = np.sqrt(final_tree_mse)
final_tree_r2 = r2_score(pptest_labels,final_tree_predictions)
final_tree_mae = mean_absolute_error(pptest_labels,final_tree_predictions)

print(f"Mean Squared Error-\nLinear Model: {final_lin_mse}\nDecision Tree: {final_tree_mse}\n")
print(f"Root Mean Squared Error-\nLinear Model: {final_lin_rmse}\nDecision Tree: {final_tree_rmse}\n")
print(f"R-Squared Score Score-\nLinear Model: {final_lin_r2}\nDecision Tree: {final_tree_r2}\n")
print(f"Mean Absolute Error-\nLinear Model: {final_lin_mae}\nDecision Tree: {final_tree_mae}\n")


plt.scatter(final_lin_mse,1,s=100)
plt.scatter(final_tree_mse,2,s=100)
plt.xticks([10,12,14,16,18,20,22,24,26,28,30])
plt.yticks([0,1,2,3],labels=['','Linear Regression','Decision Tree',''])
plt.xlabel('Mean Squared Error (Lower is better)')
plt.show()

plt.scatter(final_lin_rmse,1,s=100)
plt.scatter(final_tree_rmse,2,s=100)
plt.xticks([2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7])
plt.yticks([0,1,2,3],labels=['','Linear Regression','Decision Tree',''])
plt.xlabel('Root Mean Squared Error (Lower is better)')
plt.show()

plt.scatter(final_lin_r2,1,s=100)
plt.scatter(final_tree_r2,2,s=100)
plt.xticks([0.8,0.82,0.84,0.86,0.88,0.9,0.92,0.94,0.96,0.98,1])
plt.yticks([0,1,2,3],labels=['','Linear Regression','Decision Tree',''])
plt.xlabel('R-Squared Score (Higher is better)')
plt.show()

plt.scatter(final_lin_mae,1,s=100)
plt.scatter(final_tree_mae,2,s=100)
plt.xticks([3,3.25,3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6])
plt.yticks([0,1,2,3],labels=['','Linear Regression','Decision Tree',''])
plt.xlabel('Mean Absolute Error (Lower is better)')
plt.show()