
Task List
----------

Experiment Framework:

    1. Confidence interval around 2D datavis

    2. Grouped barchart for runtimes by platform

    X - 3. Containerize environment for this w/ Docker

    4. 3D vis of some data?

    5. Meta-script to analyze time complexity of variably sized data

    6. Memory profiling :(

    X - 7. Reset counter, little cleanup utility



Web Service:

    1. Visualizations
        - List what Madhavan says here
        - Data size vs model accuracy plotting
        - Exploratory Data Analysis

    2. Make dispatch function to handle different user-interface requests (AI params, future work, etc)
        - expand functionality to handle these new tasks
    
    3. Pickle best model to embed on webpage
        - should be able to return to use model based on request id

    4. Set limit on k folds on backend - error if k > n, where n is rows

    In Future:
    Allow requests to run concurrently
    ReIntroduce TestBest pipeline
    Use heuristic/new model to give user information on which models/regressors to deploy for which data
    Ensemble Model
    Report to go along with results
    Embed a model in the webpage
    Make visualization package
    

    
#################################
Research Papers:
##################################


Survey of Popular Libraries

    1. Comparison Experiments
        - runtime for different devices across libraries for (lst sqs?)(ANN?)
        - memory useage for different devices across libraries for (lst sqs?)(ANN?)

    2. Add tables to describe scope of libraries (Sam)
        - Regressors
        X - activation functions
        - evaluation metrics
        - layer types
        - optimizers (need Josh to locate optimizers in OpenCV before I can mark complete)
        - loss functions

    3. Synthesize information to fit 6 pages

    4. Fix citations



Current Implementations of Least Squares

    1. Complete styling for experiment figures

    2. Redo Ellipsoid Experiment
        - generate data (# of dimensions: 2,5,10,50,100,250,500...10000)
        - generate data (rotations for data of various size [~5 datasets])
        - conduct experiment
        - writeup results
        - add report and figures to LaTex doc

    3. Big-O Experiment
        - research time complexities mentioned in documentation (if it exists)
        - find suitably large dataset
        - perform big-O analysis for each library w figures
        - writeup results
        - add report and figures to LaTex doc
    
    4. MinMax Data
        X - generate data
        - conduct experiment
        - writeup results
        - add report and figures to LaTex doc

    5. New Experiments
        - ask for other interesting datasets at next meeting
        - if not supplied, select from beta dataset list
        - repeat experiment pipeline

    6. Lst Sqrs literature review 

    7. Write Paper


Data-Centric MLaaS

    1. Write Paper
