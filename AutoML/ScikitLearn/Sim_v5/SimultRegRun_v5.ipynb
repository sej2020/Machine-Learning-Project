{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First 20 Models Simultaneous Run**\n",
    "\n",
    "General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import all_estimators\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Importing All Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_regs() -> list:\n",
    "    \"\"\"\n",
    "    This function imports all sklearn regression estimators. The function will filter all out all regressors\n",
    "    that perform poorly or take additional parameters. It will return a list of all\n",
    "    viable regressor classes and a list of the names of all the viable regressor classes. \n",
    "    \n",
    "    \"\"\"\n",
    "    estimators = all_estimators(type_filter='regressor')\n",
    "    forbidden_estimators = [\n",
    "        \"DummyRegressor\", \"GaussianProcessRegressor\", \"KernelRidge\", \n",
    "        \"QuantileRegressor\", \"SGDRegressor\", \n",
    "        \"MultiOutputRegressor\", \"RegressorChain\",\n",
    "        \"StackingRegressor\", \"VotingRegressor\"\n",
    "        ]\n",
    "    all_regs = []\n",
    "    all_reg_names = []\n",
    "    for name, RegressorClass in estimators:\n",
    "        if name not in forbidden_estimators:\n",
    "                print('Appending', name)\n",
    "                reg = RegressorClass()\n",
    "                all_regs.append(reg)\n",
    "                all_reg_names.append(name)\n",
    "        else:\n",
    "            print(f\"Skipping {name}\")\n",
    "    return all_regs, all_reg_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test Split and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datapath) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will take the relative file path of a csv file and return a pandas DataFrame of the csv content.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.abspath(datapath)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def create_strat_cat(raw_data) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will add a categorical column to the dataframe. This column is the categorical representation of the class\n",
    "    label of each instance. This will enable the data to be split according to the distribution of the class values. The appended\n",
    "    dataframe will be returned.\n",
    "    \"\"\"\n",
    "    strat_label = raw_data.columns[-1]\n",
    "    description = raw_data.describe()\n",
    "    strat_bins = list(description.loc['min':'max',strat_label])\n",
    "    strat_bins[0], strat_bins[-1] = -np.inf, np.inf\n",
    "    raw_data[f\"{strat_label}_cat\"] = pd.cut(raw_data[strat_label],bins=strat_bins,labels=[1,2,3,4])\n",
    "    data_w_strat_cat = raw_data\n",
    "    return data_w_strat_cat, strat_label\n",
    "\n",
    "def data_split(data, strat_label) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will split the data into training attributes, training labels, test attributes and test labels according\n",
    "    to the distribution of a categorical class label.\n",
    "    \"\"\"\n",
    "    split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "    for train_index, test_index in split.split(data,data[f\"{strat_label}_cat\"]):\n",
    "        train_set = data.loc[train_index]\n",
    "        test_set = data.loc[test_index]\n",
    "    for set_ in(train_set,test_set):\n",
    "        set_.drop(f\"{strat_label}_cat\",axis=1,inplace=True)\n",
    "    train = train_set.copy()\n",
    "    test = test_set.copy()\n",
    "\n",
    "    data_label = train.columns[-1]\n",
    "    train_attrib = train.drop(data_label,axis=1)\n",
    "    train_labels = train[data_label].copy()\n",
    "    test_attrib = test.drop(data_label,axis=1)\n",
    "    test_labels = test[data_label].copy()\n",
    "\n",
    "    return train_attrib, train_labels, test_attrib, test_labels\n",
    "\n",
    "def scale(train_attrib, train_labels, test_attrib, test_labels) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will perform standardization feature scaling on training instances and test instances of a dataset.\n",
    "    It will return the scaled training attributes, the training labels, the scaled test attributes, and the test labels.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train_attrib = scaler.fit_transform(train_attrib)\n",
    "    scaled_test_attrib = scaler.fit_transform(test_attrib)\n",
    "    return scaled_train_attrib, train_labels, scaled_test_attrib, test_labels\n",
    "\n",
    "def data_transform(datapath) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will take a relative datapath of a dataset in csv format and return preprocessed training attributes,  \n",
    "    training labels, test attributes, and test labels of the dataset.\n",
    "    \"\"\"\n",
    "    raw_data = load_data(datapath)\n",
    "    data_w_strat_cat, strat_label = create_strat_cat(raw_data)\n",
    "    split_data = data_split(data_w_strat_cat, strat_label)\n",
    "    train_attrib, train_labels, test_attrib, test_labels = scale(*split_data)\n",
    "    return train_attrib, train_labels, test_attrib, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneous Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(datapath, n_regressors, metric_list, n_vizualized, metric_help, score_method='neg_mean_squared_errror') -> None:\n",
    "    \"\"\"\n",
    "    This function will perform cross-validation training across multiple regressor types for one dataset. \n",
    "    The cross-validation scores will be vizualized in a box plot chart, displaying regressor performance across\n",
    "    specified metrics. These charts will be saved to the user's CPU as a png file. The best performing model \n",
    "    trained on each regressor type will be tested on the set of test instances. The performance of those regs \n",
    "    on the test instances will be recorded in a table and saved to the user's CPU as a png file.\n",
    "    \"\"\"\n",
    "    regs, reg_names = get_all_regs()\n",
    "    if n_regressors != 'all':\n",
    "        regs, reg_names = regs[0:n_regressors], reg_names[0:n_regressors]\n",
    "    train_attrib, train_labels, test_attrib, test_labels = data_transform(datapath)\n",
    "    cv_data = []\n",
    "    errors = []\n",
    "    passed_regs = []\n",
    "    if score_method not in metric_list:\n",
    "        metric_list = [score_method]+metric_list\n",
    "    #training each regressor in CV\n",
    "    for i in range(len(regs)):\n",
    "        x = run(regs[i], metric_list, train_attrib, train_labels)\n",
    "        if type(x) == dict:\n",
    "            cv_data += [x]\n",
    "        else:\n",
    "            errors += [regs[i]]\n",
    "    print(f\"These regressors threw errors in CV: {errors}\")\n",
    "    #removing the names of the regressors that threw errors in CV\n",
    "    for j in range(len(regs)):\n",
    "        if regs[j] not in errors:\n",
    "            passed_regs += [reg_names[j]]\n",
    "    figs = [test_best(cv_data, passed_regs, metric_list, test_attrib, test_labels, metric_help, score_method)]\n",
    "    for metric in metric_list:\n",
    "        figs += [boxplot(cv_data, passed_regs, metric, n_vizualized, metric_help)]\n",
    "    for k in range(len(figs)):\n",
    "        figs[k].savefig(f'fig_{k}.png',bbox_inches='tight')\n",
    "    pass\n",
    "\n",
    "def run(model, metric_list, train_attrib, train_labels) -> dict:\n",
    "    \"\"\"\n",
    "    This function will perform cross-validation training on a given dataset and given regressor. It will return\n",
    "    a dictionary containing cross-validation performance on various metrics.\n",
    "    \"\"\"\n",
    "    print(f\"Checking {model}\")\n",
    "    try:\n",
    "        cv_outer = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "        cv_output_dict = cross_validate(model, train_attrib, train_labels, scoring=metric_list, cv=cv_outer, return_estimator=True)\n",
    "        return cv_output_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def boxplot(cv_data, passed_regs, metric, n_vizualized, metric_help):\n",
    "    \"\"\"\n",
    "    This function will return a box plot chart displaying the cross-validation scores of various regressors for a given metric.\n",
    "    The box plot chart will be in descending order by median performance. The chart will be saved to the user's CPU as a png file.\n",
    "    \"\"\"\n",
    "    boxfig = plt.figure(constrained_layout=True)\n",
    "    df = pd.DataFrame()\n",
    "    #Making CV scores on the specified metric positive and storing in a dataframe. Repeat for each regressor.\n",
    "    for i,j in zip(cv_data,passed_regs):\n",
    "            df[j] = list(i['test_'+metric]*metric_help[metric][1])\n",
    "    #Sorting the columns by median value of the CV scores. The metric_help dictionary helps to determine whether it will be an ascending\n",
    "    # sort or a descending sort based on the metric.\n",
    "    sorted_index = df.median().sort_values(ascending=metric_help[metric][0]).index\n",
    "    df_sorted = df_sorted=df[sorted_index]\n",
    "    #Creating box plot figure of best n regressors.\n",
    "    df_sorted.iloc[:,len(df_sorted.columns)-n_vizualized:].boxplot(vert=False,grid=False)\n",
    "    plt.xlabel(f'CV {metric}')\n",
    "    plt.ylabel('Models')\n",
    "    return boxfig\n",
    "\n",
    "def test_best(cv_data, passed_regs, metric_list, test_attrib, test_labels, metric_help, score_method):\n",
    "    \"\"\"\n",
    "    This function will take the best performing model on each regressor type generated by cross-validation training and \n",
    "    apply it to the set of test data. The performance of the regs on the test instances will be displayed on a table and\n",
    "    saved to the user's CPU as a png file. The regs will be sorted in descending order by performance on specified metrics.\n",
    "    \"\"\"\n",
    "    #initializing a nested list to store the scores of each model on each metric when applied to the test set\n",
    "    metric_columns = []\n",
    "    for metric in metric_list:\n",
    "        metric_columns += [[metric,[]]]\n",
    "    for i in cv_data:\n",
    "        #'x' will store the list of CV scores for the given score_method metric\n",
    "        if score_method == 'neg_root_mean_squared_error':\n",
    "            x = list(np.sqrt(i['test_'+score_method]*metric_help[score_method][1]))\n",
    "        else:\n",
    "            x = list(i['test_'+score_method]*metric_help[score_method][1])\n",
    "        y = list(i['estimator'])\n",
    "        #for each score in 'x', if it is the best score, that model will be stored in the 'best' variable\n",
    "        for j in range(len(x)):\n",
    "            if metric_help[score_method][0] == True:\n",
    "                if x[j] == max(x):\n",
    "                    best = y[j]\n",
    "            else:\n",
    "                if x[j] == min(x):\n",
    "                    best = y[j]\n",
    "        #the best model will predict the test attributes\n",
    "        predictions = best.predict(test_attrib)\n",
    "        #the performance of the model prediction will be stored in the 'metric_columns' list\n",
    "        # 'metric_help[k[0]][2]' is the associated statistic that will measure the difference between the test labels\n",
    "        #  and the prediction of the best model\n",
    "        for k in metric_columns:\n",
    "            if k[0] == 'neg_root_mean_squared_error':\n",
    "                k[1] += [round(np.sqrt(metric_help[k[0]][2](test_labels,predictions)),4)]\n",
    "            else:\n",
    "                k[1] += [round(metric_help[k[0]][2](test_labels,predictions),4)]\n",
    "    #preparing dataframe. column names will be the metrics used. the row labels will be the regressors\n",
    "    columnnames = metric_list\n",
    "    final_columns = []\n",
    "    for m in metric_columns:\n",
    "        final_columns += [m[1]]\n",
    "    df = pd.DataFrame(np.array(final_columns).T,index=passed_regs,columns=columnnames)\n",
    "    sorted_df = df.sort_values(by=metric_list[0],ascending=metric_help[metric][0])\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    ax.table(cellText=sorted_df.values, rowLabels=sorted_df.index, colLabels=sorted_df.columns, loc='center')\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramdict = {'datapath': 'AutoML/PowerPlantData/Folds5x2_pp.csv',\n",
    "            'n_regressors': 'all',\n",
    "            'metric_list': ['neg_mean_squared_error','neg_mean_absolute_error','r2'],\n",
    "            'n_vizualized': 20,\n",
    "            'metric_help': {'explained_variance': [True, 1, metrics.explained_variance_score], 'max_error': [False, 1, metrics.max_error],\n",
    "                            'neg_mean_absolute_error': [False, -1, metrics.mean_absolute_error], 'neg_mean_squared_error': [False, -1, metrics.mean_squared_error],\n",
    "                            'neg_root_mean_squared_error': [False, -1, metrics.mean_squared_error], 'neg_mean_squared_log_error': [False, -1, metrics.mean_squared_log_error],\n",
    "                            'neg_median_absolute_error': [False, -1, metrics.median_absolute_error], 'r2': [True, 1, metrics.r2_score],\n",
    "                            'neg_mean_poisson_deviance': [False, -1, metrics.mean_poisson_deviance], 'neg_mean_gamma_deviance': [False, -1, metrics.mean_gamma_deviance],\n",
    "                            'neg_mean_absolute_percentage_error': [False, -1, metrics.mean_absolute_percentage_error], 'd2_absolute_error_score': [True, 1, metrics.d2_absolute_error_score],\n",
    "                            'd2_pinball_score': [True, 1, metrics.d2_pinball_score], 'd2_tweedie_score': [True, 1, metrics.d2_tweedie_score]\n",
    "                            },\n",
    "            'score_method': 'neg_root_mean_squared_error'\n",
    "            }\n",
    "\n",
    "comparison(**paramdict)\n",
    "\n",
    "#GENERAL FORM of metric_help: {'metric':[ higher score is better? , positive or negative score values, accociated stat function ] } "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e13265c55b2786eb2a2fa7b7b4618c00aafbb52a034e4f1759946ea06f2e244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
